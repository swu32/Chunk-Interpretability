{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ce99df-0f87-4fc1-b075-7084afff65c0",
   "metadata": {},
   "source": [
    "collect hidden states activity from \n",
    "* T5\n",
    "* mamba \n",
    "* RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0cf7100-6672-45df-a14f-b473bbb0f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "prompt_dir = './prompt_bank.json'\n",
    "with open(prompt_dir, 'r') as file:\n",
    "    prompt_bank = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b79082-47e2-45d8-b52d-0ef7cb62802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/swu/miniforge3/envs/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_llama3(model_id=\"meta-llama/Meta-Llama-3-8B\", device=device): # 33 layers, 4096 embedding dimension\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_T5(model_id=\"t5-small\", device=device): # 7 encoder layer, 512 embedding dimension \n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_mamba(model_id=\"state-spaces/mamba-130m-hf\", device=device): # mamba has 25 layers and  768 embedding dimension\n",
    "    # Load the model and tokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = MambaForCausalLM.from_pretrained(model_id).to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_RWKV(model_id= \"RWKV/rwkv-4-169m-pile\", device = device): # RWKV has 13 layers\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    return model, tokenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b091b98-56f6-40f5-8a6a-1d896a9b7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sentence_by_sentence_data(model, tokenizer, prompt_bank, device):\n",
    "    for key in prompt_bank.keys():    \n",
    "        paragraph = prompt_bank[key]\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "        if model_name == 'llama3': nlayer=33\n",
    "        elif model_name == 'T5': nlayer=7\n",
    "        elif model_name == 'mamba': nlayer = 25\n",
    "        elif model_name == 'rwkv': nlayer = 13\n",
    "        hidden_states_accumulated =[[] for _ in range(nlayer)] # This will be a list of lists, one list per layer\n",
    "        for sentence in sentences:\n",
    "            input_text = sentence\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "            # Forward pass through the model and access hidden states\n",
    "            if model_name == 'llama3':\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "                hidden_states = outputs.hidden_states  # Tuple of hidden states at each layer\n",
    "\n",
    "            elif model_name == 'T5':\n",
    "                with torch.no_grad():\n",
    "                    encoder_outputs = model.encoder(\n",
    "                        input_ids=inputs.input_ids,\n",
    "                        attention_mask=inputs.attention_mask,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True)\n",
    "                # Get hidden states from all encoder layers\n",
    "                encoder_hidden_states = encoder_outputs.hidden_states  # Tuple of length num_layers + 1\n",
    "                last_encoder_hidden = encoder_hidden_states[-1]  # shape: (batch_size, seq_len, hidden_dim)\n",
    "                # Start token for T5 decoder is usually </s> (id=1)\n",
    "                decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]], device = device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(\n",
    "                        input_ids=inputs.input_ids,\n",
    "                        attention_mask=inputs.attention_mask,\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                \n",
    "                hidden_states = outputs.encoder_hidden_states\n",
    "                #decoder_hidden_states = outputs.decoder_hidden_states\n",
    "                # print(len(encoder_hidden_states))# len(hidden_states) = 7 # n_layer \n",
    "                # print(encoder_hidden_states[0].shape) # (1, sequence_length, embedding_dim=512)\n",
    "                \n",
    "            elif model_name == 'mamba':# mamba has 25 layers and the embedding dimension of 768\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states  # Tuple of hidden states at each layer\n",
    "                print(len(hidden_states))\n",
    "            elif model_name == 'rwkv': \n",
    "                with torch.no_grad():\n",
    "                   outputs = model(**inputs, output_hidden_states=True)\n",
    "                hidden_states = outputs.hidden_states  # Tuple of hidden states at each layer\n",
    "                print(len(hidden_states))# 13 layers \n",
    "                \n",
    "            print(hidden_states[0].shape) # len(hidden_states) = 33 # n_layer \n",
    "            # hidden_states[0].shape # (1, sequence_length, embedding_dim)\n",
    "            \n",
    "            for i, hidden_state in enumerate(hidden_states):\n",
    "                # Detach, move to CPU, and append the hidden state\n",
    "                hidden_state_np = hidden_state.detach().cpu().float().numpy()\n",
    "                hidden_states_accumulated[i].append(hidden_state_np)\n",
    "\n",
    "        # Concatenate hidden states across all sentences for each layer\n",
    "        for i, layer_states in enumerate(hidden_states_accumulated):\n",
    "            # Concatenate along the sequence length (axis 1)\n",
    "            concatenated_hidden_state = np.concatenate(layer_states, axis=1)  # Shape: (batch_size, total_sequence_length, hidden_size)\n",
    "            \n",
    "            # Save the concatenated hidden state\n",
    "            np.save(f'./hidden_unit_activity/{key}_concatenated_hidden_state_layer_{i}_model={model_name}.npy', concatenated_hidden_state)\n",
    "            print(f\"Layer {i} concatenated hidden state shape: {concatenated_hidden_state.shape}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c55b9-53d1-4458-a273-1586cb9569a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['rwkv']:#'llama3','T5','mamba',\n",
    "    if model_name == 'llama3':\n",
    "        model, tokenizer = load_llama3(model_id=\"meta-llama/Meta-Llama-3-8B\", device=device)\n",
    "    if model_name == 'T5':\n",
    "        model, tokenizer = load_T5(model_id=\"t5-small\", device=device)\n",
    "    if model_name == 'mamba':\n",
    "        model, tokenizer = load_mamba(model_id=\"state-spaces/mamba-130m-hf\", device=device)\n",
    "    if model_name == 'rwkv':\n",
    "        model, tokenizer = load_RWKV(model_id= \"RWKV/rwkv-4-169m-pile\", device = device)\n",
    "        \n",
    "    collect_sentence_by_sentence_data(model, tokenizer, prompt_bank, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
