{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119c0d7c-bf2a-47be-ad11-23d8ffebd4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/swu/miniforge3/envs/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# Log in with your API token\n",
    "login(token=\"YOURTOKENHERE\")\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set CUDA deterministic mode\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(model_id=\"meta-llama/Meta-Llama-3-8B\", device=\"cuda\"):\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    return model, tokenizer\n",
    "model, tokenizer = load_model(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9967563-36c2-4603-a60b-79566302dbf8",
   "metadata": {},
   "source": [
    "# grafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc349432-80b1-4fdc-8633-b7a9f975e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text and the target token\n",
    "input_text = \"Hi, how are you doing?\"\n",
    "# Tokenize input and identify the token position to analyze\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a669c8-0aba-4882-b0ce-842e18683c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Perturbation: What shall we make today?  I have a few ideas, but I’m open to suggestions.  I’m thinking of making a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)\n",
    "\n",
    "# Generate text before perturbation\n",
    "attention_mask = input_tokens[\"attention_mask\"]\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token if pad_token is not set\n",
    "\n",
    "output_before = model.generate(\n",
    "    input_ids=input_tokens[\"input_ids\"],\n",
    "    attention_mask=attention_mask,                # Provide the attention mask\n",
    "    max_length=30,\n",
    "    pad_token_id=tokenizer.pad_token_id,          # Explicitly set pad token ID\n",
    "    temperature=0.0,                              # Set temperature to 0 for deterministic output\n",
    "    do_sample=False,\n",
    "    num_beams=1,# Disable sampling to ensure deterministic behavior\n",
    ")\n",
    "\n",
    "print(\"Before Perturbation:\", tokenizer.decode(output_before[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206bf66-efb3-42f5-aa21-c1dd2c0b97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./neural_chunk_dictionary/neural_chunk_dictionary.pkl\", \"rb\") as file:\n",
    "    neural_chunk_dictionary = pickle.load(file)\n",
    "\n",
    "hook_counter = 0\n",
    "\n",
    "def create_modify_input_hook(constant_values=None, constant_positions=None, token_idx=3):\n",
    "    \"\"\"Returns a hook function that modifies the input during the forward pass.\"\"\"\n",
    "    \n",
    "    def modify_input_hook(module, input):\n",
    "        \"\"\"\n",
    "        Hook to modify the input during the forward pass.\n",
    "        Args:\n",
    "            module: The layer being hooked.\n",
    "            input: The inputs to the layer.\n",
    "        Returns:\n",
    "            Modified input.\n",
    "        \"\"\"\n",
    "        modified_input = input[0].clone() # shape = 1 x 8 x 4096\n",
    "        \n",
    "        if modified_input.shape[1]>1:\n",
    "            modified_input[:, token_idx, constant_positions] = constant_values #shape  torch.Size([1, 8, 4096])\n",
    "            \n",
    "        global hook_counter\n",
    "        hook_counter += 1\n",
    "        return (modified_input,) # I don't understand why the second time the modified input takes the shape of 1 x 4096\n",
    "        \n",
    "    return modify_input_hook\n",
    "\n",
    "word = 'cake'\n",
    "print('grafting word = ', word)\n",
    "\n",
    "# Register pre-hooks on specific layers\n",
    "hooks = []  # To store references to the hooks\n",
    "token_idx = 6\n",
    "\n",
    "for layer_to_perturb in range(2,31):  \n",
    "    # Register the pre-hook on the forward pass of the layer\n",
    "    constant_values = neural_chunk_dictionary[word]['layer'][layer_to_perturb]['constant_values']\n",
    "    constant_positions = neural_chunk_dictionary[word]['layer'][layer_to_perturb]['constant_positions'][:,1] # perturb so the network thinks that it is a cake, instead of cheese cake \n",
    "    #print(constant_values)\n",
    "    #print(constant_positions)\n",
    "    constant_positions_tensor = torch.tensor(constant_positions, dtype=torch.int64, device=device)\n",
    "    constant_values_tensor = torch.tensor(constant_values,dtype=torch.bfloat16, device=device)\n",
    "\n",
    "    transformer_layers = model.model.layers[layer_to_perturb].input_layernorm.to(device)\n",
    "    hook = transformer_layers.register_forward_pre_hook(create_modify_input_hook(constant_values=constant_values_tensor, constant_positions=constant_positions_tensor, token_idx=token_idx))\n",
    "    hooks.append(hook)\n",
    "    \n",
    "inputs = input_tokens.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_after = model.generate(input_tokens['input_ids'], \n",
    "                                  max_length=200,\n",
    "                                  attention_mask=attention_mask, \n",
    "                                  pad_token_id=tokenizer.pad_token_id)\n",
    "    print(len(output_after[0]))\n",
    "    print(\"After Perturbation:\", tokenizer.decode(output_after[0], skip_special_tokens=True))\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks: hook.remove()\n",
    "# # Access the modified hidden states\n",
    "# hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5182645-4cf1-49bf-b57a-25608f6ebfa9",
   "metadata": {},
   "source": [
    "# freezing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e0ac71-9e1f-48c7-9aee-88fad56d0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text and the target token\n",
    "input_text = \"What is the name of a rich, savory food made from curdled milk, often aged to enhance its flavors? \"\n",
    "# Tokenize input and identify the token position to analyze\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5869230-0a23-497f-92db-cfbfc35a3db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Perturbation: What is the name of a rich, savory food made from curdled milk, often aged to enhance its flavors?  It is a staple of many cuisines, and is often used as a condiment or ingredient in other dishes.  It is also a popular ingredient in many desserts.  What is it?\n",
      "The answer is cheese.  Cheese is a dairy product made from the curdled milk of various animals, including cows, goats, and sheep.  It is a staple of many cuisines, and is often used as a condiment or ingredient in other dishes.  It is also a popular ingredient in many desserts.\n",
      "Cheese is made by curdling milk with an acid, such as lemon juice or vinegar.  The curds are then separated from the whey, and the curds are pressed into blocks or wheels.  The curds are then aged to enhance their flavors.  The aging process can take\n"
     ]
    }
   ],
   "source": [
    "# Generate text before perturbation\n",
    "attention_mask = input_tokens[\"attention_mask\"]\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token if pad_token is not set\n",
    "# Generate output with the correct pad_token_id and attention_mask\n",
    "output_before = model.generate(\n",
    "    input_ids=input_tokens[\"input_ids\"],\n",
    "    attention_mask=attention_mask,                # Provide the attention mask\n",
    "    max_length=200,\n",
    "    pad_token_id=tokenizer.pad_token_id,           # Explicitly set pad token ID\n",
    "    temperature=0.0,                              # Set temperature to 0 for deterministic output\n",
    "    do_sample=False,\n",
    "    num_beams=1,# Disable sampling to ensure deterministic behavior\n",
    ")\n",
    "print(\"Before Perturbation:\", tokenizer.decode(output_before[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a631ea-b010-4c9f-af9a-eecf9529b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./neural_chunk_dictionary/neural_chunk_dictionary.pkl\", \"rb\") as file:\n",
    "    neural_chunk_dictionary = pickle.load(file)\n",
    "\n",
    "hook_counter = 0\n",
    "\n",
    "def create_modify_input_hook(constant_values=None, constant_positions=None, token_idx=3):\n",
    "    \"\"\"Returns a hook function that modifies the input during the forward pass.\"\"\"\n",
    "    \n",
    "    def modify_input_hook(module, input):\n",
    "        \"\"\"\n",
    "        Hook to modify the input during the forward pass.\n",
    "        Args:\n",
    "            module: The layer being hooked.\n",
    "            input: The inputs to the layer.\n",
    "        Returns:\n",
    "            Modified input.\n",
    "        \"\"\"\n",
    "        modified_input = input[0].clone() # shape = 1 x 8 x 4096\n",
    "        \n",
    "        if modified_input.shape[1]>1:\n",
    "            modified_input[:, token_idx, constant_positions] = 0 \n",
    "            \n",
    "        global hook_counter\n",
    "        hook_counter += 1\n",
    "        return (modified_input,) # I don't understand why the second time the modified input takes the shape of 1 x 4096\n",
    "        \n",
    "    return modify_input_hook\n",
    "\n",
    "word = 'cheese'\n",
    "print('word = ', word)\n",
    "\n",
    "# Register pre-hooks on specific layers\n",
    "hooks = []  # To store references to the hooks\n",
    "token_idx = 7\n",
    "\n",
    "for layer_to_perturb in range(1,32):  \n",
    "    # Register the pre-hook on the forward pass of the layer\n",
    "    constant_values = neural_chunk_dictionary[word]['layer'][layer_to_perturb]['constant_values']\n",
    "    constant_positions = neural_chunk_dictionary[word]['layer'][layer_to_perturb]['constant_positions'][:,1] # perturb so the network thinks that it is a cake, instead of cheese cake \n",
    "\n",
    "    constant_positions_tensor = torch.tensor(constant_positions, dtype=torch.int64, device=device)\n",
    "    constant_values_tensor = torch.tensor(constant_values,dtype=torch.bfloat16, device=device)\n",
    "\n",
    "    transformer_layers = model.model.layers[layer_to_perturb].input_layernorm.to(device)\n",
    "    hook = transformer_layers.register_forward_pre_hook(create_modify_input_hook(constant_values=constant_values_tensor, constant_positions=constant_positions_tensor, token_idx=token_idx))\n",
    "    hooks.append(hook)\n",
    "    \n",
    "inputs = input_tokens.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_before = model.generate(\n",
    "    input_ids=input_tokens[\"input_ids\"],\n",
    "    attention_mask=attention_mask,                # Provide the attention mask\n",
    "    max_length=200,\n",
    "    pad_token_id=tokenizer.pad_token_id,          # Explicitly set pad token ID\n",
    "    temperature=0.0,                              # Set temperature to 0 for deterministic output\n",
    "    do_sample=False,\n",
    "    num_beams=1,# Disable sampling to ensure deterministic behavior\n",
    ")\n",
    "    print(len(output_after[0]))\n",
    "    print(\"After Perturbation:\", tokenizer.decode(output_after[0], skip_special_tokens=True))\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks: hook.remove()\n",
    "# # Access the modified hidden states\n",
    "# hidden_states = outputs.hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
