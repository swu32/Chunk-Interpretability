{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a851a89-e0c5-45bd-97a0-56206efcdf17",
   "metadata": {},
   "source": [
    "# evaluate grafting effectiveness across a range of TREC contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1f4ef-6fe2-4561-aa21-a65749d197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./neural_chunk_dictionary/neural_chunk_dictionary.pkl\", \"rb\") as file:\n",
    "    neural_chunk_dictionary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f12cf-5121-4268-81fb-17322bf05eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"trec\")\n",
    "\n",
    "# Show coarse label categories\n",
    "coarse_labels = dataset[\"train\"].features['coarse_label'].names\n",
    "print(\"Coarse label categories:\")\n",
    "for i, label in enumerate(coarse_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "# Show fine-grained label categories\n",
    "fine_labels = dataset[\"train\"].features[\"fine_label\"].names\n",
    "print(\"\\nFine-grained label categories:\")\n",
    "for i, label in enumerate(fine_labels):\n",
    "    print(f\"{i}: {label}\")\n",
    "\n",
    "# Label\tName\tMeaning / Question Type Examples\n",
    "# 0\tABBR\tAbbreviation – Questions asking about acronyms or abbreviations.\n",
    "# \"What does HTML stand for?\"\n",
    "# 1\tDESC\tDescription / Definition – Asking for definitions, explanations, or descriptions.\n",
    "# \"What is photosynthesis?\"\n",
    "# 2\tENTY\tEntity – Questions asking about a thing or object (e.g., color, currency, food).\n",
    "# \"What is the capital of France's currency?\"\n",
    "# 3\tHUM\tHuman – Asking about people or groups.\n",
    "# \"Who discovered America?\"\n",
    "# 4\tLOC\tLocation – Questions about places, countries, cities, etc.\n",
    "# \"Where is the Eiffel Tower?\"\n",
    "# 5\tNUM\tNumeric – Questions that expect a number as an answer (e.g., date, size, price).\n",
    "# \"How many people live in Japan?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6250d54-1373-4935-83b8-ef7bdb945e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_it = 100\n",
    "batched_prompts = []\n",
    "batched_labels = []\n",
    "for i in range(0, n_it):\n",
    "    batched_prompts.append(dataset[\"train\"][i]['text'])\n",
    "    batched_labels.append(dataset[\"train\"][i]['coarse_label']) # [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701b9a9e-462c-4912-b7cc-ad2f503fb40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=\"YOURTOKENHERE\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def load_model(model_id=\"meta-llama/Meta-Llama-3-8B\", device=\"cuda\"):\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load llama model and the associated tokenizer\n",
    "model, tokenizer = load_model(device=device)\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf1a286-c7ed-44f1-a682-5a645e4b40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response_batch(input_texts):\n",
    "    \"\"\"\n",
    "    input_texts: List[str] – a list of prompts\n",
    "    returns: List[str] – a list of generated outputs (one per input)\n",
    "    \"\"\"\n",
    "    assert isinstance(input_texts, list), \"input_texts should be a list of strings\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize with padding\n",
    "    input_tokens = tokenizer(\n",
    "        input_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_tokens['input_ids'],\n",
    "            attention_mask=input_tokens['attention_mask'],\n",
    "            max_length=200,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.5,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # Decode batch\n",
    "    decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    return decoded_outputs\n",
    "\n",
    "\n",
    "def load_dictionary_and_perturb_batch(word='cake', token_idx=6, input_texts=None):\n",
    "    \"\"\"\n",
    "    input_texts: List[str]\n",
    "    Returns: List[str] of generated sentences after perturbation\n",
    "    \"\"\"\n",
    "    assert isinstance(input_texts, list), \"input_texts should be a list of strings\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize batch\n",
    "    input_tokens = tokenizer(\n",
    "        input_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = input_tokens[\"attention_mask\"]\n",
    "    batch_size, seq_len = input_tokens[\"input_ids\"].shape\n",
    "\n",
    "    # Hook factory\n",
    "    def create_modify_input_hook(constant_values=None, constant_positions=None, token_idx=3):\n",
    "        def modify_input_hook(module, input):\n",
    "            modified_input = input[0].clone()  # shape = [B, T, D]\n",
    "            if modified_input.shape[1] > token_idx:\n",
    "                # modified_input[:, token_idx, constant_positions] = constant_values\n",
    "                modified_input[:, token_idx-3:token_idx, constant_positions] = constant_values\n",
    "\n",
    "            return (modified_input,)\n",
    "        return modify_input_hook\n",
    "\n",
    "    # Register hooks for each layer\n",
    "    hooks = []\n",
    "    if 'layer' in neural_chunk_dictionary[word]:\n",
    "        layerchunk = neural_chunk_dictionary[word]['layer']\n",
    "    else:\n",
    "        layerchunk = neural_chunk_dictionary[word][0]['layer']# sometimes an extra timestep parameter \n",
    "\n",
    "    for layer_to_perturb in range(2, 10):\n",
    "        constant_values = layerchunk[layer_to_perturb-1]['constant_values']\n",
    "        constant_positions = layerchunk[layer_to_perturb-1]['constant_positions'][:, 1]\n",
    "\n",
    "        constant_positions_tensor = torch.tensor(constant_positions, dtype=torch.int64, device=device)\n",
    "        constant_values_tensor = torch.tensor(constant_values, dtype=torch.bfloat16, device=device)\n",
    "\n",
    "        hook = model.model.layers[layer_to_perturb].register_forward_pre_hook(\n",
    "            create_modify_input_hook(\n",
    "                constant_values=constant_values_tensor,\n",
    "                constant_positions=constant_positions_tensor,\n",
    "                token_idx=token_idx\n",
    "            )\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Generate output with perturbation\n",
    "    with torch.no_grad():\n",
    "        output_after_perturb = model.generate(\n",
    "            input_ids=input_tokens['input_ids'],\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=200,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # Clean up hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Decode batch\n",
    "    return tokenizer.batch_decode(output_after_perturb, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dictionary_and_freeze_batch(word='cake', token_idx=6, input_texts=None):\n",
    "    \"\"\"\n",
    "    input_texts: List[str]\n",
    "    Freezes specific dimensions of activations to zero at token_idx across layers 2–9.\n",
    "    Returns: List[str] – generated outputs after freezing\n",
    "    \"\"\"\n",
    "    assert isinstance(input_texts, list), \"input_texts must be a list of strings\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize batch\n",
    "    input_tokens = tokenizer(\n",
    "        input_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = input_tokens[\"attention_mask\"]\n",
    "\n",
    "    # Load dictionary\n",
    "    with open(\"./neural_chunk_dictionary/neural_chunk_dictionary.pkl\", \"rb\") as file:\n",
    "        neural_chunk_dictionary = pickle.load(file)\n",
    "\n",
    "    # Hook factory to zero out chunk dimensions\n",
    "    def create_freeze_hook(constant_positions=None, token_idx=3):\n",
    "        def modify_input_hook(module, input):\n",
    "            modified_input = input[0].clone()  # [B, T, D]\n",
    "            if modified_input.shape[1] > token_idx:\n",
    "                modified_input[:, token_idx-3:token_idx, constant_positions] = 0  # freeze those dims\n",
    "            return (modified_input,)\n",
    "        return modify_input_hook\n",
    "\n",
    "    # Register hooks for layers 2 to 9\n",
    "    hooks = []\n",
    "    for layer_to_perturb in range(2, 10):\n",
    "        constant_positions = neural_chunk_dictionary[word]['layer'][layer_to_perturb - 1]['constant_positions'][:, 1]\n",
    "        constant_positions_tensor = torch.tensor(constant_positions, dtype=torch.int64, device=device)\n",
    "        hook = model.model.layers[layer_to_perturb].register_forward_pre_hook(\n",
    "            create_freeze_hook(constant_positions=constant_positions_tensor, token_idx=token_idx)\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Generate with freezing\n",
    "    with torch.no_grad():\n",
    "        output_after = model.generate(\n",
    "            input_ids=input_tokens['input_ids'],\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=200,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.5\n",
    "        )\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return tokenizer.batch_decode(output_after, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e222f47-f6a2-4fbf-8063-2887b8805638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# === Load TREC dataset ===\n",
    "dataset = load_dataset(\"trec\", split=\"train\")\n",
    "coarse_label_names = dataset.features[\"coarse_label\"].names\n",
    "label_ids = dataset[\"coarse_label\"]\n",
    "questions = dataset[\"text\"]\n",
    "\n",
    "# === Build category → prompt mapping ===\n",
    "category_to_prompts = defaultdict(list)\n",
    "for text, label_id in zip(questions, label_ids):\n",
    "    category = coarse_label_names[label_id]\n",
    "    category_to_prompts[category].append(text)\n",
    "\n",
    "#print(category_to_prompts)\n",
    "# === Sampling parameters ===\n",
    "n_it = 5  # number of prompts per category\n",
    "categories = list(category_to_prompts.keys())\n",
    "\n",
    "# === Load chunk dictionary ===\n",
    "with open(\"./neural_chunk_dictionary.pkl\", \"rb\") as file:\n",
    "    neural_chunk_dictionary = pickle.load(file)\n",
    "\n",
    "words = list(neural_chunk_dictionary.keys())\n",
    "\n",
    "# === Run per-category perturbation analysis ===\n",
    "results = []\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\n--- Processing category: {category} ---\")\n",
    "    batched_prompts = random.sample(category_to_prompts[category], min(n_it, len(category_to_prompts[category])))\n",
    "\n",
    "    for word in words:\n",
    "        if isinstance(word, str):\n",
    "            print(f'>>> Processing word: {word}')\n",
    "\n",
    "            # Get control outputs\n",
    "            control_outputs = get_model_response_batch(batched_prompts)\n",
    "\n",
    "            # Perturbed outputs\n",
    "            perturb_outputs = load_dictionary_and_perturb_batch(\n",
    "                word=word, token_idx=-1, input_texts=batched_prompts\n",
    "            )\n",
    "\n",
    "            # Match pattern\n",
    "            pattern = re.compile(rf\"\\b{re.escape(word)}\\b\", re.IGNORECASE)\n",
    "            n_control = sum(bool(pattern.search(out)) for out in control_outputs)\n",
    "            n_perturb = sum(bool(pattern.search(out)) for out in perturb_outputs)\n",
    "\n",
    "            # Compute probabilities\n",
    "            p_control = n_control / len(batched_prompts)\n",
    "            p_perturb = n_perturb / len(batched_prompts)\n",
    "\n",
    "            # Try to get chunk category\n",
    "            chunk_category = neural_chunk_dictionary[word].get(\"category\", \"Unknown\")\n",
    "\n",
    "            # Save to results list\n",
    "            results.append({\n",
    "                \"Target Word\": word,\n",
    "                \"TREC_Category\": category,\n",
    "                \"Condition\": \"Without Perturbation (Control)\",\n",
    "                \"Occurrence Probability\": p_control,\n",
    "                \"Chunk Category\": chunk_category,\n",
    "            })\n",
    "            results.append({\n",
    "                \"Target Word\": word,\n",
    "                \"TREC_Category\": category,\n",
    "                \"Condition\": \"With Perturbation\",\n",
    "                \"Occurrence Probability\": p_perturb,\n",
    "                \"Chunk Category\": chunk_category,\n",
    "            })\n",
    "\n",
    "# === Save to DataFrame ===\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"perturbation_effectiveness_by_trec_category.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8e052-8963-4b66-9389-2c437a7c2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df = pd.read_csv(\"perturbation_effectiveness_by_trec_category_d=e_niter=50.csv\")\n",
    "\n",
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Optionally, prevent line wrapping\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a84e3c-fe3b-4f2d-94b2-1482e3c26e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e200d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e200d_level0_col0\" class=\"col_heading level0 col0\" >TREC_Category</th>\n",
       "      <th id=\"T_e200d_level0_col1\" class=\"col_heading level0 col1\" >With Perturbation</th>\n",
       "      <th id=\"T_e200d_level0_col2\" class=\"col_heading level0 col2\" >Without Perturbation (Control)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row0_col0\" class=\"data row0 col0\" >ABBR</td>\n",
       "      <td id=\"T_e200d_row0_col1\" class=\"data row0 col1\" >0.559 ± 0.324</td>\n",
       "      <td id=\"T_e200d_row0_col2\" class=\"data row0 col2\" >0.149 ± 0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row1_col0\" class=\"data row1 col0\" >DESC</td>\n",
       "      <td id=\"T_e200d_row1_col1\" class=\"data row1 col1\" >0.490 ± 0.268</td>\n",
       "      <td id=\"T_e200d_row1_col2\" class=\"data row1 col2\" >0.156 ± 0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row2_col0\" class=\"data row2 col0\" >ENTY</td>\n",
       "      <td id=\"T_e200d_row2_col1\" class=\"data row2 col1\" >0.481 ± 0.286</td>\n",
       "      <td id=\"T_e200d_row2_col2\" class=\"data row2 col2\" >0.126 ± 0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row3_col0\" class=\"data row3 col0\" >HUM</td>\n",
       "      <td id=\"T_e200d_row3_col1\" class=\"data row3 col1\" >0.467 ± 0.286</td>\n",
       "      <td id=\"T_e200d_row3_col2\" class=\"data row3 col2\" >0.119 ± 0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row4_col0\" class=\"data row4 col0\" >LOC</td>\n",
       "      <td id=\"T_e200d_row4_col1\" class=\"data row4 col1\" >0.475 ± 0.286</td>\n",
       "      <td id=\"T_e200d_row4_col2\" class=\"data row4 col2\" >0.107 ± 0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e200d_row5_col0\" class=\"data row5 col0\" >NUM</td>\n",
       "      <td id=\"T_e200d_row5_col1\" class=\"data row5 col1\" >0.453 ± 0.270</td>\n",
       "      <td id=\"T_e200d_row5_col2\" class=\"data row5 col2\" >0.115 ± 0.184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f330bd4d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group and compute mean and std\n",
    "grouped = (\n",
    "    df.groupby([\"TREC_Category\", \"Condition\"])[\"Occurrence Probability\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create \"mean ± std\" strings\n",
    "grouped[\"Mean ± Std\"] = grouped.apply(\n",
    "    lambda row: f\"{row['mean']:.3f} ± {row['std']:.3f}\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot table to get conditions as columns\n",
    "pivoted = grouped.pivot(index=\"TREC_Category\", columns=\"Condition\", values=\"Mean ± Std\")\n",
    "\n",
    "# Reset index to make it look nice\n",
    "pivoted = pivoted.reset_index()\n",
    "pivoted.columns.name = None  # remove pandas-generated name\n",
    "pivoted\n",
    "display(pivoted.style.hide(axis=\"index\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f984fff-2e1f-4125-a030-771a89080058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5ef6b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5ef6b_level0_col0\" class=\"col_heading level0 col0\" >TREC_Category</th>\n",
       "      <th id=\"T_5ef6b_level0_col1\" class=\"col_heading level0 col1\" >With Perturbation</th>\n",
       "      <th id=\"T_5ef6b_level0_col2\" class=\"col_heading level0 col2\" >Without Perturbation (Control)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row0_col0\" class=\"data row0 col0\" >ABBR</td>\n",
       "      <td id=\"T_5ef6b_row0_col1\" class=\"data row0 col1\" >0.308 ± 0.269</td>\n",
       "      <td id=\"T_5ef6b_row0_col2\" class=\"data row0 col2\" >0.149 ± 0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row1_col0\" class=\"data row1 col0\" >DESC</td>\n",
       "      <td id=\"T_5ef6b_row1_col1\" class=\"data row1 col1\" >0.281 ± 0.224</td>\n",
       "      <td id=\"T_5ef6b_row1_col2\" class=\"data row1 col2\" >0.156 ± 0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row2_col0\" class=\"data row2 col0\" >ENTY</td>\n",
       "      <td id=\"T_5ef6b_row2_col1\" class=\"data row2 col1\" >0.225 ± 0.241</td>\n",
       "      <td id=\"T_5ef6b_row2_col2\" class=\"data row2 col2\" >0.126 ± 0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row3_col0\" class=\"data row3 col0\" >HUM</td>\n",
       "      <td id=\"T_5ef6b_row3_col1\" class=\"data row3 col1\" >0.215 ± 0.238</td>\n",
       "      <td id=\"T_5ef6b_row3_col2\" class=\"data row3 col2\" >0.119 ± 0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row4_col0\" class=\"data row4 col0\" >LOC</td>\n",
       "      <td id=\"T_5ef6b_row4_col1\" class=\"data row4 col1\" >0.205 ± 0.224</td>\n",
       "      <td id=\"T_5ef6b_row4_col2\" class=\"data row4 col2\" >0.107 ± 0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ef6b_row5_col0\" class=\"data row5 col0\" >NUM</td>\n",
       "      <td id=\"T_5ef6b_row5_col1\" class=\"data row5 col1\" >0.218 ± 0.214</td>\n",
       "      <td id=\"T_5ef6b_row5_col2\" class=\"data row5 col2\" >0.115 ± 0.184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f330bd4d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"perturbation_effectiveness_by_trec_category_d=m_niter=50.csv\")\n",
    "\n",
    "\n",
    "# Group and compute mean and std\n",
    "grouped = (\n",
    "    df.groupby([\"TREC_Category\", \"Condition\"])[\"Occurrence Probability\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create \"mean ± std\" strings\n",
    "grouped[\"Mean ± Std\"] = grouped.apply(\n",
    "    lambda row: f\"{row['mean']:.3f} ± {row['std']:.3f}\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot table to get conditions as columns\n",
    "pivoted = grouped.pivot(index=\"TREC_Category\", columns=\"Condition\", values=\"Mean ± Std\")\n",
    "\n",
    "# Reset index to make it look nice\n",
    "pivoted = pivoted.reset_index()\n",
    "pivoted.columns.name = None  # remove pandas-generated name\n",
    "pivoted\n",
    "display(pivoted.style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87168b-b55b-424d-bf89-4ad5b6a687c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"perturbation_effectiveness_by_trec_category_d=e_niter=50.csv\")\n",
    "\n",
    "# Group by Target Word and Condition, averaging across TREC categories\n",
    "grouped = (\n",
    "    df.groupby([\"Target Word\", \"Condition\"])[\"Occurrence Probability\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort alphabetically by Target Word\n",
    "grouped = grouped.sort_values(by=\"Target Word\")\n",
    "\n",
    "# Display result\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a5cd6-dcdc-4085-997a-7a59b508ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"perturbation_effectiveness_by_trec_category_d=e_niter=50.csv\")\n",
    "\n",
    "# Group by Target Word and Condition, averaging across TREC categories\n",
    "grouped = (\n",
    "    df.groupby([\"Target Word\", \"Condition\"])[\"Occurrence Probability\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot so each Target Word has one row with both conditions as columns\n",
    "pivoted = grouped.pivot(index=\"Target Word\", columns=\"Condition\", values=\"Occurrence Probability\")\n",
    "\n",
    "# Optional: clean up column names\n",
    "pivoted = pivoted.reset_index()\n",
    "pivoted.columns.name = None  # remove index name\n",
    "\n",
    "# Sort alphabetically by Target Word\n",
    "pivoted = pivoted.sort_values(by=\"Target Word\")\n",
    "\n",
    "# Show result\n",
    "pivoted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354ae76-58f9-478c-badf-cccb87d37568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"perturbation_effectiveness_by_trec_category_d=e_niter=50.csv\")\n",
    "\n",
    "# Group by Target Word and Condition, averaging across TREC categories\n",
    "grouped = (\n",
    "    df.groupby([\"Target Word\", \"Condition\"])[\"Occurrence Probability\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot so each Target Word has one row with both conditions as columns\n",
    "pivoted = grouped.pivot(index=\"Target Word\", columns=\"Condition\", values=\"Occurrence Probability\")\n",
    "pivoted.columns.name = None  # clean column names\n",
    "pivoted = pivoted.reset_index()\n",
    "\n",
    "# Calculate delta\n",
    "pivoted[\"Delta (Perturb - Control)\"] = (\n",
    "    pivoted[\"With Perturbation\"] - pivoted[\"Without Perturbation (Control)\"]\n",
    ")\n",
    "\n",
    "# Sort alphabetically by Target Word\n",
    "pivoted = pivoted.sort_values(by=\"Delta (Perturb - Control)\", ascending=False)\n",
    "\n",
    "# Display result\n",
    "pivoted\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
